# Model Training

This document outlines the process of training a machine learning model using the prepared Million Song Dataset.

## Table of Contents
1. [Prerequisites](#prerequisites)
2. [Configuration](#configuration)
3. [Data Loading](#data-loading)
4. [Model Selection](#model-selection)
5. [Model Training](#model-training)
6. [Hyperparameter Tuning](#hyperparameter-tuning)
7. [Model Evaluation](#model-evaluation)
8. [Experiment Tracking with MLflow](#experiment-tracking-with-mlflow)

## Prerequisites

Ensure you have the following installed:
- Python 3.8+
- Required packages:
  - pandas
  - numpy
  - scikit-learn
  - mlflow
  - boto3
  - pyarrow

Install the required packages:

```bash
pip install -r requirements.txt
```

## Configuration

The project uses a `config.json` file generated by Terraform. This file contains important configuration parameters. Ensure this file is present in the project root directory before proceeding.

## Data Loading

Data is loaded using the `prepare_data` function from `src/data/load_data.py`. This function handles:

1. Loading data from S3 or local Parquet file
2. Cleaning the data
3. Feature engineering
4. Preparing features and target variables

## Model Selection

For this project, we're using a Random Forest Regressor. The model is defined in `src/models/train_model.py`:

```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
```

## Model Training

To train the model, run the following command from the project root:

```bash
python src/models/train_model.py
```

This script will:
1. Load and prepare the data
2. Split the data into training and test sets
3. Train the Random Forest model
4. Evaluate the model on the test set
5. Log the experiment details to MLflow

## Hyperparameter Tuning

For hyperparameter tuning, we can use scikit-learn's GridSearchCV. Here's an example of how to implement this:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
```

Add this code to `train_model.py` if you want to perform hyperparameter tuning.

## Model Evaluation

The model is evaluated using Mean Squared Error (MSE) and R-squared (R2) score. These metrics are calculated in `train_model.py`:

```python
from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2}")
```

## Experiment Tracking with MLflow

We use MLflow to track experiments. The `train_model.py` script logs the following to MLflow:

- Parameters: model type, number of estimators
- Metrics: MSE, R2 score
- Model: The trained model is logged for easy retrieval
- Feature Importance: Importance of each feature is logged as a metric

To view the MLflow UI, run:

```bash
mlflow ui
```

Then navigate to `http://localhost:5000` in your web browser.

## Next Steps

After training and evaluating your model:

1. Review the model's performance and feature importance in the MLflow UI.
2. If satisfied with the model's performance, proceed to the [Model Deployment](deployment.md) guide.
3. If the model's performance needs improvement, consider:
   - Feature engineering: Create new features or transform existing ones.
   - Trying different algorithms: Test other models like Gradient Boosting or Neural Networks.
   - Collecting more data: If possible, increase the dataset size.

Remember to version your data and code to ensure reproducibility of your experiments.
